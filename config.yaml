# TMF921 Intent Translation Research Configuration

project:
  name: "TMF921 Intent Translation"
  version: "0.1.0"
  description: "Research project for translating natural language intents to TMF921 using lightweight LLMs"

data:
  scenarios_path: "scenarios.json"
  gst_spec_path: "gst.json"
  specifications_dir: "All_PDFs"
  train_split: 0.70
  val_split: 0.15
  test_split: 0.15
  random_seed: 42

models:
  # Lightweight models for local deployment
  ollama:
    enabled: true
    base_url: "http://localhost:11434"
    models:
      # Cloud models (faster, datacenter hardware)
      - name: "gpt-oss:20b-cloud"
        alias: "gpt-oss-20b-cloud"
        cloud: true
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 4096
      - name: "gpt-oss:120b-cloud"
        alias: "gpt-oss-120b-cloud"
        cloud: true
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 4096
      
      # Local models
      - name: "llama3:latest"
        alias: "llama-3-8b"
        cloud: false
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 4096
      - name: "llama3.1:8b"
        alias: "llama-3.1-8b"
        cloud: false
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 4096
      - name: "phi3:mini"
        alias: "phi-3-mini"
        cloud: false
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 4096
      - name: "gemma2:9b"
        alias: "gemma-2-9b"
        cloud: false
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 4096

experiments:
  # Quick validation settings
  quick_validation:
    num_samples: 10
    models: ["phi-3-mini"]  # Using smaller model due to RAM constraints
    
  # Phase 3: Baseline
  zero_shot:
    num_samples: 50
    use_structured_prompt: true
    prompt_sections: 5
    
  # Phase 4: Few-shot
  few_shot:
    num_examples: [1, 3, 5]
    num_samples: 50
    selection_strategy: "similarity"  # random, similarity, diverse
    
  # Phase 5: RAG
  rag:
    enabled: true
    num_samples: 50
    chunk_size: 512
    chunk_overlap: 50
    top_k_retrieval: 3
    use_knowledge_graph: true
    
  # Phase 5: CoT
  chain_of_thought:
    num_samples: 30
    reasoning_steps: ["understand", "extract", "map", "generate", "validate"]
    
  # Phase 6: Fine-tuning (optional)
  finetune:
    enabled: false
    lora_rank: 8
    lora_alpha: 16
    batch_size: 4
    epochs: 3

evaluation:
  # FEACI metrics (Dinh et al., 2025)
  metrics:
    - "format_correctness"
    - "explainability"
    - "accuracy"
    - "cost"
    - "inference_time"
  
  # Hallucination detection
  hallucination_checks:
    - "bandwidth_plausibility"
    - "latency_plausibility"
    - "characteristic_dependencies"
    - "schema_compliance"
  
  # Human evaluation sample size
  human_eval_sample: 20

output:
  results_dir: "results"
  experiments_dir: "experiments"
  logs_dir: "logs"
  cache_dir: ".cache"
  save_raw_outputs: true
  save_failed_cases: true
