{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TMF921 Intent Translation - Interactive Notebook\n",
                "\n",
                "This notebook provides an interactive environment for experimenting with TMF921 intent translation using lightweight LLMs.\n",
                "\n",
                "## Features\n",
                "- Test individual scenarios\n",
                "- Compare different approaches (few-shot, RAG)\n",
                "- Visualize results\n",
                "- Experiment with prompts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "First, let's import all necessary modules and initialize components."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "# Add src to path\n",
                "sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
                "\n",
                "from tmf921 import (\n",
                "    ScenarioDataset, GSTSpecification, TMF921Validator, OllamaClient,\n",
                "    TMF921PromptBuilder, EXAMPLE_SCENARIOS,\n",
                "    GSTRetriever, CharacteristicNameMapper\n",
                ")\n",
                "\n",
                "print(\"✓ All modules imported successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data and initialize components\n",
                "print(\"Loading components...\")\n",
                "\n",
                "# Load GST specification\n",
                "gst = GSTSpecification(\"gst.json\")\n",
                "print(f\"✓ Loaded GST with {len(gst.spec['serviceSpecCharacteristic'])} characteristics\")\n",
                "\n",
                "# Load scenarios\n",
                "scenarios = ScenarioDataset(\"data/val.json\")\n",
                "print(f\"✓ Loaded {len(scenarios.scenarios)} validation scenarios\")\n",
                "\n",
                "# Initialize components\n",
                "validator = TMF921Validator(gst.spec)\n",
                "prompt_builder = TMF921PromptBuilder(gst.spec)\n",
                "name_mapper = CharacteristicNameMapper(gst.spec)\n",
                "\n",
                "print(\"✓ All components initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Test Single Scenario Translation\n",
                "\n",
                "Let's test translating a single scenario with different approaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose a test scenario\n",
                "test_scenario = \"Deploy IoT sensor network: 1 Mbps per device, 500ms latency tolerance, 99.9% availability, 10,000 sensors.\"\n",
                "\n",
                "print(f\"Test Scenario: {test_scenario}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Initialize LLM Client"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize client (choose model)\n",
                "MODEL = \"gpt-oss:20b-cloud\"  # or \"llama3:latest\" for local\n",
                "\n",
                "client = OllamaClient(model=MODEL)\n",
                "\n",
                "if client._check_connection():\n",
                "    print(f\"✓ Connected to Ollama with model: {MODEL}\")\n",
                "else:\n",
                "    print(\"✗ Cannot connect to Ollama. Make sure it's running.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Approach 1: Few-Shot Learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build few-shot prompt\n",
                "system_prompt = prompt_builder.build_system_prompt()\n",
                "user_prompt = prompt_builder.build_few_shot_prompt(\n",
                "    test_scenario,\n",
                "    EXAMPLE_SCENARIOS[:3],\n",
                "    max_examples=3\n",
                ")\n",
                "\n",
                "print(f\"Prompt length: {len(user_prompt)} characters\\n\")\n",
                "print(\"First 500 characters:\")\n",
                "print(user_prompt[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate translation\n",
                "import time\n",
                "\n",
                "print(\"Generating translation...\")\n",
                "start = time.time()\n",
                "\n",
                "response = client.generate(\n",
                "    prompt=user_prompt,\n",
                "    system_prompt=system_prompt,\n",
                "    temperature=0.1\n",
                ")\n",
                "\n",
                "elapsed = time.time() - start\n",
                "\n",
                "print(f\"✓ Generated in {elapsed:.1f}s\")\n",
                "print(f\"Tokens: {response['tokens']}\")\n",
                "\n",
                "# Extract JSON\n",
                "intent_json = client.extract_json(response['response'])\n",
                "\n",
                "if intent_json:\n",
                "    print(\"\\n✓ Successfully extracted JSON\")\n",
                "    print(json.dumps(intent_json, indent=2))\n",
                "else:\n",
                "    print(\"\\n✗ Failed to extract JSON\")\n",
                "    print(response['response'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate result\n",
                "if intent_json:\n",
                "    # Apply name correction\n",
                "    corrected_intent, corrections = name_mapper.correct_intent(intent_json)\n",
                "    \n",
                "    # Validate\n",
                "    validation = validator.validate_all(corrected_intent)\n",
                "    \n",
                "    print(\"Validation Results:\")\n",
                "    print(f\"  Format valid: {validation['format_valid']}\")\n",
                "    print(f\"  Characteristics valid: {validation['characteristics_valid']}\")\n",
                "    print(f\"  Overall valid: {validation['overall_valid']}\")\n",
                "    print(f\"  Name corrections: {len(corrections)}\")\n",
                "    \n",
                "    if corrections:\n",
                "        print(f\"\\nCorrections applied:\")\n",
                "        for correction in corrections:\n",
                "            print(f\"  - {correction}\")\n",
                "    \n",
                "    if not validation['overall_valid']:\n",
                "        print(f\"\\nErrors:\")\n",
                "        for error in validation['errors']:\n",
                "            print(f\"  - {error}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Approach 2: RAG-Enhanced"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize RAG retriever\n",
                "try:\n",
                "    retriever = GSTRetriever()\n",
                "    print(\"✓ RAG retriever initialized\")\n",
                "    \n",
                "    # Retrieve relevant characteristics\n",
                "    retrieved = retriever.retrieve_for_scenario(test_scenario, n_results=8)\n",
                "    \n",
                "    print(f\"\\nRetrieved {len(retrieved)} characteristics:\")\n",
                "    for i, char in enumerate(retrieved, 1):\n",
                "        print(f\"  {i}. {char['name']} (similarity: {char['similarity']:.3f})\")\n",
                "        \nexcept Exception as e:\n",
                "    print(f\"✗ RAG not available: {e}\")\n",
                "    print(\"  Run: python scripts/setup_rag.py\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build RAG prompt (if retriever works)\n",
                "if 'retrieved' in locals() and retrieved:\n",
                "    rag_prompt = prompt_builder.build_rag_prompt(\n",
                "        test_scenario,\n",
                "        retrieved,\n",
                "        include_examples=True\n",
                "    )\n",
                "    \n",
                "    print(f\"RAG prompt length: {len(rag_prompt)} characters\")\n",
                "    \n",
                "    # Generate with RAG\n",
                "    print(\"\\nGenerating with RAG...\")\n",
                "    start = time.time()\n",
                "    \n",
                "    response_rag = client.generate(\n",
                "        prompt=rag_prompt,\n",
                "        system_prompt=system_prompt,\n",
                "        temperature=0.1\n",
                "    )\n",
                "    \n",
                "    elapsed = time.time() - start\n",
                "    print(f\"✓ Generated in {elapsed:.1f}s\")\n",
                "    \n",
                "    # Extract and validate\n",
                "    intent_rag = client.extract_json(response_rag['response'])\n",
                "    \n",
                "    if intent_rag:\n",
                "        validation_rag = validator.validate_all(intent_rag)\n",
                "        print(f\"\\nRAG Result - Valid: {validation_rag['overall_valid']}\")\n",
                "        print(json.dumps(intent_rag, indent=2)[:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Batch Testing\n",
                "\n",
                "Test multiple scenarios and analyze results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select test scenarios\n",
                "test_scenarios = scenarios.scenarios[:5]  # First 5 scenarios\n",
                "\n",
                "results = []\n",
                "\n",
                "print(f\"Testing {len(test_scenarios)} scenarios...\\n\")\n",
                "\n",
                "for i, scenario in enumerate(test_scenarios, 1):\n",
                "    print(f\"[{i}/{len(test_scenarios)}] {scenario[:60]}...\")\n",
                "    \n",
                "    try:\n",
                "        # Build prompt (few-shot)\n",
                "        user_prompt = prompt_builder.build_few_shot_prompt(\n",
                "            scenario, EXAMPLE_SCENARIOS[:2], max_examples=2\n",
                "        )\n",
                "        \n",
                "        # Generate\n",
                "        response = client.generate(\n",
                "            prompt=user_prompt,\n",
                "            system_prompt=system_prompt,\n",
                "            temperature=0.1\n",
                "        )\n",
                "        \n",
                "        # Extract and validate\n",
                "        intent = client.extract_json(response['response'])\n",
                "        \n",
                "        if intent:\n",
                "            corrected, _ = name_mapper.correct_intent(intent)\n",
                "            validation = validator.validate_all(corrected)\n",
                "            \n",
                "            results.append({\n",
                "                'scenario': scenario,\n",
                "                'valid': validation['overall_valid'],\n",
                "                'time': response['time_seconds'],\n",
                "                'tokens': response['tokens']\n",
                "            })\n",
                "            \n",
                "            status = \"✓\" if validation['overall_valid'] else \"✗\"\n",
                "            print(f\"  {status} Valid: {validation['overall_valid']} ({response['time_seconds']:.1f}s)\")\n",
                "        else:\n",
                "            print(f\"  ✗ Failed to extract JSON\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"  ✗ Error: {str(e)[:50]}\")\n",
                "\n",
                "print(f\"\\nCompleted: {len(results)}/{len(test_scenarios)} successful\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze batch results\n",
                "if results:\n",
                "    valid_count = sum(1 for r in results if r['valid'])\n",
                "    avg_time = sum(r['time'] for r in results) / len(results)\n",
                "    avg_tokens = sum(r['tokens'] for r in results) / len(results)\n",
                "    \n",
                "    print(\"Batch Results Summary:\")\n",
                "    print(f\"  Accuracy: {valid_count}/{len(results)} ({valid_count/len(results)*100:.0f}%)\")\n",
                "    print(f\"  Avg Time: {avg_time:.1f}s per scenario\")\n",
                "    print(f\"  Avg Tokens: {avg_tokens:.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prompt Experimentation\n",
                "\n",
                "Test different prompt variations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom scenario for testing\n",
                "custom_scenario = \"Create a smart city traffic management system: 100 Mbps bandwidth, 10ms latency, 99.99% uptime.\"\n",
                "\n",
                "print(f\"Custom Scenario: {custom_scenario}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test with different number of examples\n",
                "for num_examples in [0, 1, 3]:\n",
                "    print(f\"\\n--- Testing with {num_examples} examples ---\")\n",
                "    \n",
                "    if num_examples == 0:\n",
                "        # Zero-shot\n",
                "        prompt = prompt_builder.build_zero_shot_prompt(custom_scenario)\n",
                "    else:\n",
                "        # Few-shot\n",
                "        prompt = prompt_builder.build_few_shot_prompt(\n",
                "            custom_scenario,\n",
                "            EXAMPLE_SCENARIOS[:num_examples],\n",
                "            max_examples=num_examples\n",
                "        )\n",
                "    \n",
                "    print(f\"Prompt length: {len(prompt)} chars\")\n",
                "    \n",
                "    # You can uncomment to actually run\n",
                "    # response = client.generate(prompt=prompt, system_prompt=system_prompt)\n",
                "    # print(f\"Result: {response['response'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualize Results\n",
                "\n",
                "Create visualizations of experiment results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and compare experiment results\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "experiments_to_compare = [\n",
                "    \"validation_50\",\n",
                "    \"rag_cloud_50_scenarios\"\n",
                "]\n",
                "\n",
                "experiment_results = {}\n",
                "\n",
                "for exp in experiments_to_compare:\n",
                "    try:\n",
                "        with open(f\"results/{exp}/metrics_summary.json\") as f:\n",
                "            experiment_results[exp] = json.load(f)\n",
                "    except:\n",
                "        print(f\"Could not load {exp}\")\n",
                "\n",
                "if experiment_results:\n",
                "    # Create comparison plots\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "    \n",
                "    # Accuracy comparison\n",
                "    names = list(experiment_results.keys())\n",
                "    accuracies = [experiment_results[n]['feaci']['accuracy'] for n in names]\n",
                "    \n",
                "    axes[0].bar(names, accuracies)\n",
                "    axes[0].set_ylabel('Accuracy (%)')\n",
                "    axes[0].set_title('Accuracy Comparison')\n",
                "    axes[0].set_ylim([0, 100])\n",
                "    \n",
                "    # Speed comparison\n",
                "    speeds = [experiment_results[n]['feaci']['inference_time_avg_seconds'] for n in names]\n",
                "    \n",
                "    axes[1].bar(names, speeds)\n",
                "    axes[1].set_ylabel('Time (seconds)')\n",
                "    axes[1].set_title('Speed Comparison')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No experiment results to visualize\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Export Results\n",
                "\n",
                "Save your experimental results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save notebook results\n",
                "if results:\n",
                "    output_file = \"notebook_results.json\"\n",
                "    \n",
                "    with open(output_file, 'w') as f:\n",
                "        json.dump(results, f, indent=2)\n",
                "    \n",
                "    print(f\"✓ Results saved to {output_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "- Run full experiments: `python scripts/run_experiment.py --list`\n",
                "- Analyze results: `python scripts/analyze_results.py --list`\n",
                "- See README.md for more information"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}